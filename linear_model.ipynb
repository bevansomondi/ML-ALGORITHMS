{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanillaGrad(object):\n",
    "    def __init__(self,weight_shape, learning_rate=0.1, C=0, loss_function=\"MSE\"):\n",
    "        self.weights = np.array(np.random.randn(weight_shape), ndmin=2)\n",
    "        self.lr = learning_rate\n",
    "        self.loss = loss_function\n",
    "        self.history = {}\n",
    "        self.C = C# parameter that controls the amount of regulaarization\n",
    "\n",
    "    def update_w(self, X, y):\n",
    "        \"\"\" The default loss function is the preferred Mean Squared error, the other alternative is the MAE\"\"\"\n",
    "        dl_dw = 0\n",
    "        \n",
    "        \n",
    "        N = len(X)\n",
    "            #looping over all the training data\n",
    "        for i in range(N):\n",
    "            x = np.array(X[i], ndmin=2).T\n",
    "            np.dot(self.weights, x)  \n",
    "\n",
    "            if self.loss == \"MSE\":                            \n",
    "                dl_dw += (-2 *x * (y[i] - np.dot(self.weights, x)))\n",
    "                \n",
    "            \n",
    "            #to use absolute loss function in future \n",
    "            else:\n",
    "                 print(\"Only MSE allowed for now\")\n",
    "                \n",
    "        #update the weight matrix\n",
    "        #dividing by N to average the loss\n",
    "        self.weights -= 1/N * (self.lr * dl_dw.T)\n",
    "\n",
    "        return self.weights\n",
    "\n",
    "    def average_loss(self, X, y):\n",
    "            X = np.array(X, ndmin=2)\n",
    "            total_error = 0.0\n",
    "            N = len(X)\n",
    "            for i in range(N):\n",
    "                x = X[i].T\n",
    "                if self.loss == \"MSE\":\n",
    "                    total_error += (y[i] - np.dot(self.weights, x)) **2\n",
    "                #to use absolute loss funct in future\n",
    "                else:\n",
    "                     print(\"Only MSE allowed for now\")\n",
    "                \n",
    "            return self.C * np.sum(self.weights)*-1 + total_error / float(N)\n",
    "\n",
    "        \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "                #update the weights\n",
    "                self.update_w(X, y)\n",
    "                #calculate the loss for the epoch \n",
    "                self.history[epoch] = self.average_loss(X, y)\n",
    "\n",
    "                print(f\"Epoch {epoch} / {epochs}: loss {self.history[epoch]}\")\n",
    "    \n",
    "    def test(self, X,y):\n",
    "         #return the MSE\n",
    "         MSE = self.average_loss(X, y)\n",
    "         return MSE\n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating data \n",
    "X = np.random.rand(500, 4)\n",
    "y = []\n",
    "# y = 2x1 + 3x2\n",
    "for x in X:\n",
    "    y.append(2*x[0] + 3*x[1] + x[2] + 0.001*x[3])\n",
    "\n",
    "#split the dataset into train and test set \n",
    "X_train, X_test, y_train, y_test = X[:70], X[70:], y[:70], y[70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of the model\n",
    "network = vanillaGrad(4, 0.3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 200: loss [-3.01807536]\n",
      "Epoch 1 / 200: loss [-3.00600247]\n",
      "Epoch 2 / 200: loss [-3.00111772]\n",
      "Epoch 3 / 200: loss [-2.99922389]\n",
      "Epoch 4 / 200: loss [-2.99855154]\n",
      "Epoch 5 / 200: loss [-2.99837132]\n",
      "Epoch 6 / 200: loss [-2.99838542]\n",
      "Epoch 7 / 200: loss [-2.9984729]\n",
      "Epoch 8 / 200: loss [-2.99858493]\n",
      "Epoch 9 / 200: loss [-2.99870201]\n",
      "Epoch 10 / 200: loss [-2.99881652]\n",
      "Epoch 11 / 200: loss [-2.99892569]\n",
      "Epoch 12 / 200: loss [-2.99902866]\n",
      "Epoch 13 / 200: loss [-2.99912536]\n",
      "Epoch 14 / 200: loss [-2.99921599]\n",
      "Epoch 15 / 200: loss [-2.99930088]\n",
      "Epoch 16 / 200: loss [-2.99938035]\n",
      "Epoch 17 / 200: loss [-2.99945476]\n",
      "Epoch 18 / 200: loss [-2.99952441]\n",
      "Epoch 19 / 200: loss [-2.99958962]\n",
      "Epoch 20 / 200: loss [-2.99965066]\n",
      "Epoch 21 / 200: loss [-2.99970782]\n",
      "Epoch 22 / 200: loss [-2.99976132]\n",
      "Epoch 23 / 200: loss [-2.99981141]\n",
      "Epoch 24 / 200: loss [-2.99985829]\n",
      "Epoch 25 / 200: loss [-2.99990219]\n",
      "Epoch 26 / 200: loss [-2.99994327]\n",
      "Epoch 27 / 200: loss [-2.99998173]\n",
      "Epoch 28 / 200: loss [-3.00001772]\n",
      "Epoch 29 / 200: loss [-3.0000514]\n",
      "Epoch 30 / 200: loss [-3.00008292]\n",
      "Epoch 31 / 200: loss [-3.00011241]\n",
      "Epoch 32 / 200: loss [-3.00014]\n",
      "Epoch 33 / 200: loss [-3.0001658]\n",
      "Epoch 34 / 200: loss [-3.00018994]\n",
      "Epoch 35 / 200: loss [-3.00021251]\n",
      "Epoch 36 / 200: loss [-3.00023361]\n",
      "Epoch 37 / 200: loss [-3.00025334]\n",
      "Epoch 38 / 200: loss [-3.00027177]\n",
      "Epoch 39 / 200: loss [-3.000289]\n",
      "Epoch 40 / 200: loss [-3.00030509]\n",
      "Epoch 41 / 200: loss [-3.00032012]\n",
      "Epoch 42 / 200: loss [-3.00033415]\n",
      "Epoch 43 / 200: loss [-3.00034725]\n",
      "Epoch 44 / 200: loss [-3.00035947]\n",
      "Epoch 45 / 200: loss [-3.00037087]\n",
      "Epoch 46 / 200: loss [-3.0003815]\n",
      "Epoch 47 / 200: loss [-3.00039142]\n",
      "Epoch 48 / 200: loss [-3.00040065]\n",
      "Epoch 49 / 200: loss [-3.00040926]\n",
      "Epoch 50 / 200: loss [-3.00041727]\n",
      "Epoch 51 / 200: loss [-3.00042472]\n",
      "Epoch 52 / 200: loss [-3.00043166]\n",
      "Epoch 53 / 200: loss [-3.0004381]\n",
      "Epoch 54 / 200: loss [-3.0004441]\n",
      "Epoch 55 / 200: loss [-3.00044966]\n",
      "Epoch 56 / 200: loss [-3.00045483]\n",
      "Epoch 57 / 200: loss [-3.00045962]\n",
      "Epoch 58 / 200: loss [-3.00046406]\n",
      "Epoch 59 / 200: loss [-3.00046817]\n",
      "Epoch 60 / 200: loss [-3.00047198]\n",
      "Epoch 61 / 200: loss [-3.00047551]\n",
      "Epoch 62 / 200: loss [-3.00047876]\n",
      "Epoch 63 / 200: loss [-3.00048177]\n",
      "Epoch 64 / 200: loss [-3.00048454]\n",
      "Epoch 65 / 200: loss [-3.00048709]\n",
      "Epoch 66 / 200: loss [-3.00048944]\n",
      "Epoch 67 / 200: loss [-3.0004916]\n",
      "Epoch 68 / 200: loss [-3.00049358]\n",
      "Epoch 69 / 200: loss [-3.0004954]\n",
      "Epoch 70 / 200: loss [-3.00049707]\n",
      "Epoch 71 / 200: loss [-3.00049859]\n",
      "Epoch 72 / 200: loss [-3.00049997]\n",
      "Epoch 73 / 200: loss [-3.00050123]\n",
      "Epoch 74 / 200: loss [-3.00050238]\n",
      "Epoch 75 / 200: loss [-3.00050342]\n",
      "Epoch 76 / 200: loss [-3.00050435]\n",
      "Epoch 77 / 200: loss [-3.0005052]\n",
      "Epoch 78 / 200: loss [-3.00050596]\n",
      "Epoch 79 / 200: loss [-3.00050663]\n",
      "Epoch 80 / 200: loss [-3.00050724]\n",
      "Epoch 81 / 200: loss [-3.00050777]\n",
      "Epoch 82 / 200: loss [-3.00050824]\n",
      "Epoch 83 / 200: loss [-3.00050865]\n",
      "Epoch 84 / 200: loss [-3.00050901]\n",
      "Epoch 85 / 200: loss [-3.00050932]\n",
      "Epoch 86 / 200: loss [-3.00050958]\n",
      "Epoch 87 / 200: loss [-3.0005098]\n",
      "Epoch 88 / 200: loss [-3.00050997]\n",
      "Epoch 89 / 200: loss [-3.00051011]\n",
      "Epoch 90 / 200: loss [-3.00051022]\n",
      "Epoch 91 / 200: loss [-3.0005103]\n",
      "Epoch 92 / 200: loss [-3.00051035]\n",
      "Epoch 93 / 200: loss [-3.00051037]\n",
      "Epoch 94 / 200: loss [-3.00051037]\n",
      "Epoch 95 / 200: loss [-3.00051035]\n",
      "Epoch 96 / 200: loss [-3.00051031]\n",
      "Epoch 97 / 200: loss [-3.00051026]\n",
      "Epoch 98 / 200: loss [-3.00051018]\n",
      "Epoch 99 / 200: loss [-3.0005101]\n",
      "Epoch 100 / 200: loss [-3.00051]\n",
      "Epoch 101 / 200: loss [-3.00050989]\n",
      "Epoch 102 / 200: loss [-3.00050976]\n",
      "Epoch 103 / 200: loss [-3.00050963]\n",
      "Epoch 104 / 200: loss [-3.0005095]\n",
      "Epoch 105 / 200: loss [-3.00050935]\n",
      "Epoch 106 / 200: loss [-3.0005092]\n",
      "Epoch 107 / 200: loss [-3.00050904]\n",
      "Epoch 108 / 200: loss [-3.00050888]\n",
      "Epoch 109 / 200: loss [-3.00050872]\n",
      "Epoch 110 / 200: loss [-3.00050855]\n",
      "Epoch 111 / 200: loss [-3.00050838]\n",
      "Epoch 112 / 200: loss [-3.00050821]\n",
      "Epoch 113 / 200: loss [-3.00050803]\n",
      "Epoch 114 / 200: loss [-3.00050786]\n",
      "Epoch 115 / 200: loss [-3.00050768]\n",
      "Epoch 116 / 200: loss [-3.00050751]\n",
      "Epoch 117 / 200: loss [-3.00050734]\n",
      "Epoch 118 / 200: loss [-3.00050716]\n",
      "Epoch 119 / 200: loss [-3.00050699]\n",
      "Epoch 120 / 200: loss [-3.00050682]\n",
      "Epoch 121 / 200: loss [-3.00050665]\n",
      "Epoch 122 / 200: loss [-3.00050648]\n",
      "Epoch 123 / 200: loss [-3.00050631]\n",
      "Epoch 124 / 200: loss [-3.00050615]\n",
      "Epoch 125 / 200: loss [-3.00050599]\n",
      "Epoch 126 / 200: loss [-3.00050583]\n",
      "Epoch 127 / 200: loss [-3.00050567]\n",
      "Epoch 128 / 200: loss [-3.00050551]\n",
      "Epoch 129 / 200: loss [-3.00050536]\n",
      "Epoch 130 / 200: loss [-3.00050521]\n",
      "Epoch 131 / 200: loss [-3.00050506]\n",
      "Epoch 132 / 200: loss [-3.00050492]\n",
      "Epoch 133 / 200: loss [-3.00050478]\n",
      "Epoch 134 / 200: loss [-3.00050464]\n",
      "Epoch 135 / 200: loss [-3.00050451]\n",
      "Epoch 136 / 200: loss [-3.00050437]\n",
      "Epoch 137 / 200: loss [-3.00050424]\n",
      "Epoch 138 / 200: loss [-3.00050412]\n",
      "Epoch 139 / 200: loss [-3.00050399]\n",
      "Epoch 140 / 200: loss [-3.00050387]\n",
      "Epoch 141 / 200: loss [-3.00050376]\n",
      "Epoch 142 / 200: loss [-3.00050364]\n",
      "Epoch 143 / 200: loss [-3.00050353]\n",
      "Epoch 144 / 200: loss [-3.00050342]\n",
      "Epoch 145 / 200: loss [-3.00050331]\n",
      "Epoch 146 / 200: loss [-3.00050321]\n",
      "Epoch 147 / 200: loss [-3.00050311]\n",
      "Epoch 148 / 200: loss [-3.00050301]\n",
      "Epoch 149 / 200: loss [-3.00050291]\n",
      "Epoch 150 / 200: loss [-3.00050282]\n",
      "Epoch 151 / 200: loss [-3.00050273]\n",
      "Epoch 152 / 200: loss [-3.00050264]\n",
      "Epoch 153 / 200: loss [-3.00050256]\n",
      "Epoch 154 / 200: loss [-3.00050247]\n",
      "Epoch 155 / 200: loss [-3.00050239]\n",
      "Epoch 156 / 200: loss [-3.00050232]\n",
      "Epoch 157 / 200: loss [-3.00050224]\n",
      "Epoch 158 / 200: loss [-3.00050217]\n",
      "Epoch 159 / 200: loss [-3.00050209]\n",
      "Epoch 160 / 200: loss [-3.00050202]\n",
      "Epoch 161 / 200: loss [-3.00050196]\n",
      "Epoch 162 / 200: loss [-3.00050189]\n",
      "Epoch 163 / 200: loss [-3.00050183]\n",
      "Epoch 164 / 200: loss [-3.00050177]\n",
      "Epoch 165 / 200: loss [-3.00050171]\n",
      "Epoch 166 / 200: loss [-3.00050165]\n",
      "Epoch 167 / 200: loss [-3.00050159]\n",
      "Epoch 168 / 200: loss [-3.00050154]\n",
      "Epoch 169 / 200: loss [-3.00050149]\n",
      "Epoch 170 / 200: loss [-3.00050144]\n",
      "Epoch 171 / 200: loss [-3.00050139]\n",
      "Epoch 172 / 200: loss [-3.00050134]\n",
      "Epoch 173 / 200: loss [-3.00050129]\n",
      "Epoch 174 / 200: loss [-3.00050125]\n",
      "Epoch 175 / 200: loss [-3.00050121]\n",
      "Epoch 176 / 200: loss [-3.00050116]\n",
      "Epoch 177 / 200: loss [-3.00050112]\n",
      "Epoch 178 / 200: loss [-3.00050108]\n",
      "Epoch 179 / 200: loss [-3.00050105]\n",
      "Epoch 180 / 200: loss [-3.00050101]\n",
      "Epoch 181 / 200: loss [-3.00050098]\n",
      "Epoch 182 / 200: loss [-3.00050094]\n",
      "Epoch 183 / 200: loss [-3.00050091]\n",
      "Epoch 184 / 200: loss [-3.00050088]\n",
      "Epoch 185 / 200: loss [-3.00050085]\n",
      "Epoch 186 / 200: loss [-3.00050082]\n",
      "Epoch 187 / 200: loss [-3.00050079]\n",
      "Epoch 188 / 200: loss [-3.00050076]\n",
      "Epoch 189 / 200: loss [-3.00050073]\n",
      "Epoch 190 / 200: loss [-3.00050071]\n",
      "Epoch 191 / 200: loss [-3.00050068]\n",
      "Epoch 192 / 200: loss [-3.00050066]\n",
      "Epoch 193 / 200: loss [-3.00050063]\n",
      "Epoch 194 / 200: loss [-3.00050061]\n",
      "Epoch 195 / 200: loss [-3.00050059]\n",
      "Epoch 196 / 200: loss [-3.00050057]\n",
      "Epoch 197 / 200: loss [-3.00050055]\n",
      "Epoch 198 / 200: loss [-3.00050053]\n",
      "Epoch 199 / 200: loss [-3.00050051]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.00000033e+00, 3.00000413e+00, 9.99984729e-01, 1.01182498e-03]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "network.train(X_train, y_train, epochs=200)\n",
    "network.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-3.00000138]), array([-3.00000138]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the nodel\n",
    "network.test(X_train, y_train), network.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.98106752746345"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(network.weights**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
