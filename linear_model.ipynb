{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanillaGrad(object):\n",
    "    def __init__(self,weight_shape, learning_rate=0.1, C=0, loss_function=\"MSE\"):\n",
    "        self.weights = np.array(np.random.randn(weight_shape), ndmin=2)\n",
    "        self.lr = learning_rate\n",
    "        self.loss = loss_function\n",
    "        self.history = {}\n",
    "        self.C = C# parameter that controls the amount of regulaarization\n",
    "\n",
    "    def update_w(self, X, y):\n",
    "        \"\"\" The default loss function is the preferred Mean Squared error, the other alternative is the MAE\"\"\"\n",
    "        dl_dw = 0\n",
    "        \n",
    "        \n",
    "        N = len(X)\n",
    "            #looping over all the training data\n",
    "        for i in range(N):\n",
    "            x = np.array(X[i], ndmin=2).T\n",
    "            np.dot(self.weights, x)  \n",
    "\n",
    "            if self.loss == \"MSE\":                            \n",
    "                dl_dw += (-2 *x * (y[i] - np.dot(self.weights, x)))\n",
    "                #te derivative of the regularization term wrt weight \n",
    "                dl_dw_reg = self.C * np.sum(self.weights)\n",
    "                \n",
    "            \n",
    "            #to use absolute loss function in future \n",
    "            else:\n",
    "                 print(\"Only MSE allowed for now\")\n",
    "                \n",
    "        #update the weight matrix\n",
    "        #dividing by N to average the loss\n",
    "        self.weights -= 1/N * ((self.lr * dl_dw.T) + dl_dw_reg)\n",
    "\n",
    "        return self.weights\n",
    "\n",
    "    def average_loss(self, X, y):\n",
    "            X = np.array(X, ndmin=2)\n",
    "            total_error = 0.0\n",
    "            N = len(X)\n",
    "            for i in range(N):\n",
    "                x = X[i].T\n",
    "                if self.loss == \"MSE\":\n",
    "                    total_error += (y[i] - np.dot(self.weights, x)) **2\n",
    "                #to use absolute loss funct in future\n",
    "                else:\n",
    "                     print(\"Only MSE allowed for now\")\n",
    "                \n",
    "            return total_error / float(N)\n",
    "\n",
    "        \n",
    "    def train(self, X, y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "                #update the weights\n",
    "                self.update_w(X, y)\n",
    "                #calculate the loss for the epoch \n",
    "                self.history[epoch] = self.average_loss(X, y)\n",
    "\n",
    "                print(f\"Epoch {epoch} / {epochs}: loss {self.history[epoch]}\")\n",
    "    \n",
    "    def test(self, X,y):\n",
    "         #return the MSE\n",
    "         MSE = self.average_loss(X, y)\n",
    "         return MSE\n",
    "              \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating data \n",
    "X = np.random.rand(500, 4)\n",
    "y = []\n",
    "# y = 2x1 + 3x2\n",
    "for x in X:\n",
    "    y.append(2*x[0] + 3*x[1] + x[2] + 0.1*x[3])\n",
    "\n",
    "#split the dataset into train and test set \n",
    "X_train, X_test, y_train, y_test = X[:70], X[70:], y[:70], y[70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of the model\n",
    "network = vanillaGrad(4, 0.3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 200: loss [2.00373802]\n",
      "Epoch 1 / 200: loss [0.87710591]\n",
      "Epoch 2 / 200: loss [0.68402235]\n",
      "Epoch 3 / 200: loss [0.61029423]\n",
      "Epoch 4 / 200: loss [0.55575635]\n",
      "Epoch 5 / 200: loss [0.50770889]\n",
      "Epoch 6 / 200: loss [0.46420337]\n",
      "Epoch 7 / 200: loss [0.42464633]\n",
      "Epoch 8 / 200: loss [0.38864502]\n",
      "Epoch 9 / 200: loss [0.35586258]\n",
      "Epoch 10 / 200: loss [0.32599715]\n",
      "Epoch 11 / 200: loss [0.29877651]\n",
      "Epoch 12 / 200: loss [0.27395488]\n",
      "Epoch 13 / 200: loss [0.25131022]\n",
      "Epoch 14 / 200: loss [0.23064191]\n",
      "Epoch 15 / 200: loss [0.21176859]\n",
      "Epoch 16 / 200: loss [0.19452626]\n",
      "Epoch 17 / 200: loss [0.17876656]\n",
      "Epoch 18 / 200: loss [0.16435522]\n",
      "Epoch 19 / 200: loss [0.1511707]\n",
      "Epoch 20 / 200: loss [0.13910291]\n",
      "Epoch 21 / 200: loss [0.12805211]\n",
      "Epoch 22 / 200: loss [0.1179279]\n",
      "Epoch 23 / 200: loss [0.10864829]\n",
      "Epoch 24 / 200: loss [0.10013893]\n",
      "Epoch 25 / 200: loss [0.09233232]\n",
      "Epoch 26 / 200: loss [0.08516719]\n",
      "Epoch 27 / 200: loss [0.07858787]\n",
      "Epoch 28 / 200: loss [0.0725438]\n",
      "Epoch 29 / 200: loss [0.06698898]\n",
      "Epoch 30 / 200: loss [0.0618816]\n",
      "Epoch 31 / 200: loss [0.05718361]\n",
      "Epoch 32 / 200: loss [0.05286035]\n",
      "Epoch 33 / 200: loss [0.04888029]\n",
      "Epoch 34 / 200: loss [0.04521466]\n",
      "Epoch 35 / 200: loss [0.04183725]\n",
      "Epoch 36 / 200: loss [0.03872418]\n",
      "Epoch 37 / 200: loss [0.03585362]\n",
      "Epoch 38 / 200: loss [0.03320566]\n",
      "Epoch 39 / 200: loss [0.03076213]\n",
      "Epoch 40 / 200: loss [0.02850639]\n",
      "Epoch 41 / 200: loss [0.02642327]\n",
      "Epoch 42 / 200: loss [0.02449887]\n",
      "Epoch 43 / 200: loss [0.02272048]\n",
      "Epoch 44 / 200: loss [0.02107646]\n",
      "Epoch 45 / 200: loss [0.01955616]\n",
      "Epoch 46 / 200: loss [0.01814979]\n",
      "Epoch 47 / 200: loss [0.01684843]\n",
      "Epoch 48 / 200: loss [0.01564384]\n",
      "Epoch 49 / 200: loss [0.0145285]\n",
      "Epoch 50 / 200: loss [0.01349549]\n",
      "Epoch 51 / 200: loss [0.01253846]\n",
      "Epoch 52 / 200: loss [0.01165158]\n",
      "Epoch 53 / 200: loss [0.01082947]\n",
      "Epoch 54 / 200: loss [0.01006721]\n",
      "Epoch 55 / 200: loss [0.00936026]\n",
      "Epoch 56 / 200: loss [0.00870445]\n",
      "Epoch 57 / 200: loss [0.00809592]\n",
      "Epoch 58 / 200: loss [0.00753113]\n",
      "Epoch 59 / 200: loss [0.00700683]\n",
      "Epoch 60 / 200: loss [0.00652]\n",
      "Epoch 61 / 200: loss [0.00606786]\n",
      "Epoch 62 / 200: loss [0.00564786]\n",
      "Epoch 63 / 200: loss [0.00525764]\n",
      "Epoch 64 / 200: loss [0.00489501]\n",
      "Epoch 65 / 200: loss [0.00455796]\n",
      "Epoch 66 / 200: loss [0.00424462]\n",
      "Epoch 67 / 200: loss [0.00395328]\n",
      "Epoch 68 / 200: loss [0.00368235]\n",
      "Epoch 69 / 200: loss [0.00343036]\n",
      "Epoch 70 / 200: loss [0.00319594]\n",
      "Epoch 71 / 200: loss [0.00297784]\n",
      "Epoch 72 / 200: loss [0.00277488]\n",
      "Epoch 73 / 200: loss [0.002586]\n",
      "Epoch 74 / 200: loss [0.00241019]\n",
      "Epoch 75 / 200: loss [0.00224653]\n",
      "Epoch 76 / 200: loss [0.00209415]\n",
      "Epoch 77 / 200: loss [0.00195226]\n",
      "Epoch 78 / 200: loss [0.00182012]\n",
      "Epoch 79 / 200: loss [0.00169705]\n",
      "Epoch 80 / 200: loss [0.00158241]\n",
      "Epoch 81 / 200: loss [0.00147562]\n",
      "Epoch 82 / 200: loss [0.00137613]\n",
      "Epoch 83 / 200: loss [0.00128342]\n",
      "Epoch 84 / 200: loss [0.00119703]\n",
      "Epoch 85 / 200: loss [0.00111652]\n",
      "Epoch 86 / 200: loss [0.00104148]\n",
      "Epoch 87 / 200: loss [0.00097154]\n",
      "Epoch 88 / 200: loss [0.00090634]\n",
      "Epoch 89 / 200: loss [0.00084556]\n",
      "Epoch 90 / 200: loss [0.00078889]\n",
      "Epoch 91 / 200: loss [0.00073606]\n",
      "Epoch 92 / 200: loss [0.00068679]\n",
      "Epoch 93 / 200: loss [0.00064085]\n",
      "Epoch 94 / 200: loss [0.000598]\n",
      "Epoch 95 / 200: loss [0.00055804]\n",
      "Epoch 96 / 200: loss [0.00052077]\n",
      "Epoch 97 / 200: loss [0.00048601]\n",
      "Epoch 98 / 200: loss [0.00045358]\n",
      "Epoch 99 / 200: loss [0.00042333]\n",
      "Epoch 100 / 200: loss [0.00039511]\n",
      "Epoch 101 / 200: loss [0.00036878]\n",
      "Epoch 102 / 200: loss [0.00034422]\n",
      "Epoch 103 / 200: loss [0.0003213]\n",
      "Epoch 104 / 200: loss [0.00029992]\n",
      "Epoch 105 / 200: loss [0.00027997]\n",
      "Epoch 106 / 200: loss [0.00026135]\n",
      "Epoch 107 / 200: loss [0.00024397]\n",
      "Epoch 108 / 200: loss [0.00022775]\n",
      "Epoch 109 / 200: loss [0.00021262]\n",
      "Epoch 110 / 200: loss [0.0001985]\n",
      "Epoch 111 / 200: loss [0.00018532]\n",
      "Epoch 112 / 200: loss [0.00017302]\n",
      "Epoch 113 / 200: loss [0.00016153]\n",
      "Epoch 114 / 200: loss [0.00015081]\n",
      "Epoch 115 / 200: loss [0.00014081]\n",
      "Epoch 116 / 200: loss [0.00013147]\n",
      "Epoch 117 / 200: loss [0.00012275]\n",
      "Epoch 118 / 200: loss [0.00011462]\n",
      "Epoch 119 / 200: loss [0.00010702]\n",
      "Epoch 120 / 200: loss [9.99272649e-05]\n",
      "Epoch 121 / 200: loss [9.33067009e-05]\n",
      "Epoch 122 / 200: loss [8.71258898e-05]\n",
      "Epoch 123 / 200: loss [8.13555064e-05]\n",
      "Epoch 124 / 200: loss [7.59681931e-05]\n",
      "Epoch 125 / 200: loss [7.09384264e-05]\n",
      "Epoch 126 / 200: loss [6.62423933e-05]\n",
      "Epoch 127 / 200: loss [6.18578754e-05]\n",
      "Epoch 128 / 200: loss [5.77641411e-05]\n",
      "Epoch 129 / 200: loss [5.39418454e-05]\n",
      "Epoch 130 / 200: loss [5.03729369e-05]\n",
      "Epoch 131 / 200: loss [4.70405697e-05]\n",
      "Epoch 132 / 200: loss [4.39290233e-05]\n",
      "Epoch 133 / 200: loss [4.10236261e-05]\n",
      "Epoch 134 / 200: loss [3.83106857e-05]\n",
      "Epoch 135 / 200: loss [3.57774225e-05]\n",
      "Epoch 136 / 200: loss [3.34119086e-05]\n",
      "Epoch 137 / 200: loss [3.12030111e-05]\n",
      "Epoch 138 / 200: loss [2.91403384e-05]\n",
      "Epoch 139 / 200: loss [2.72141907e-05]\n",
      "Epoch 140 / 200: loss [2.54155137e-05]\n",
      "Epoch 141 / 200: loss [2.37358556e-05]\n",
      "Epoch 142 / 200: loss [2.21673263e-05]\n",
      "Epoch 143 / 200: loss [2.07025607e-05]\n",
      "Epoch 144 / 200: loss [1.93346828e-05]\n",
      "Epoch 145 / 200: loss [1.80572736e-05]\n",
      "Epoch 146 / 200: loss [1.68643404e-05]\n",
      "Epoch 147 / 200: loss [1.57502885e-05]\n",
      "Epoch 148 / 200: loss [1.47098945e-05]\n",
      "Epoch 149 / 200: loss [1.37382817e-05]\n",
      "Epoch 150 / 200: loss [1.28308969e-05]\n",
      "Epoch 151 / 200: loss [1.19834891e-05]\n",
      "Epoch 152 / 200: loss [1.11920891e-05]\n",
      "Epoch 153 / 200: loss [1.04529908e-05]\n",
      "Epoch 154 / 200: loss [9.76273391e-06]\n",
      "Epoch 155 / 200: loss [9.11808741e-06]\n",
      "Epoch 156 / 200: loss [8.5160344e-06]\n",
      "Epoch 157 / 200: loss [7.95375786e-06]\n",
      "Epoch 158 / 200: loss [7.42862734e-06]\n",
      "Epoch 159 / 200: loss [6.93818664e-06]\n",
      "Epoch 160 / 200: loss [6.48014217e-06]\n",
      "Epoch 161 / 200: loss [6.0523522e-06]\n",
      "Epoch 162 / 200: loss [5.65281679e-06]\n",
      "Epoch 163 / 200: loss [5.27966835e-06]\n",
      "Epoch 164 / 200: loss [4.9311629e-06]\n",
      "Epoch 165 / 200: loss [4.60567184e-06]\n",
      "Epoch 166 / 200: loss [4.30167434e-06]\n",
      "Epoch 167 / 200: loss [4.01775016e-06]\n",
      "Epoch 168 / 200: loss [3.75257301e-06]\n",
      "Epoch 169 / 200: loss [3.50490432e-06]\n",
      "Epoch 170 / 200: loss [3.27358741e-06]\n",
      "Epoch 171 / 200: loss [3.05754211e-06]\n",
      "Epoch 172 / 200: loss [2.85575963e-06]\n",
      "Epoch 173 / 200: loss [2.66729792e-06]\n",
      "Epoch 174 / 200: loss [2.49127715e-06]\n",
      "Epoch 175 / 200: loss [2.32687568e-06]\n",
      "Epoch 176 / 200: loss [2.17332617e-06]\n",
      "Epoch 177 / 200: loss [2.02991199e-06]\n",
      "Epoch 178 / 200: loss [1.89596385e-06]\n",
      "Epoch 179 / 200: loss [1.7708567e-06]\n",
      "Epoch 180 / 200: loss [1.65400679e-06]\n",
      "Epoch 181 / 200: loss [1.54486893e-06]\n",
      "Epoch 182 / 200: loss [1.44293396e-06]\n",
      "Epoch 183 / 200: loss [1.34772633e-06]\n",
      "Epoch 184 / 200: loss [1.25880192e-06]\n",
      "Epoch 185 / 200: loss [1.17574594e-06]\n",
      "Epoch 186 / 200: loss [1.098171e-06]\n",
      "Epoch 187 / 200: loss [1.0257153e-06]\n",
      "Epoch 188 / 200: loss [9.58040907e-07]\n",
      "Epoch 189 / 200: loss [8.94832235e-07]\n",
      "Epoch 190 / 200: loss [8.35794522e-07]\n",
      "Epoch 191 / 200: loss [7.80652472e-07]\n",
      "Epoch 192 / 200: loss [7.29148965e-07]\n",
      "Epoch 193 / 200: loss [6.8104386e-07]\n",
      "Epoch 194 / 200: loss [6.3611287e-07]\n",
      "Epoch 195 / 200: loss [5.94146513e-07]\n",
      "Epoch 196 / 200: loss [5.5494914e-07]\n",
      "Epoch 197 / 200: loss [5.18338015e-07]\n",
      "Epoch 198 / 200: loss [4.84142467e-07]\n",
      "Epoch 199 / 200: loss [4.5220309e-07]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.99954219, 2.99820976, 1.00210426, 0.1004956 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "network.train(X_train, y_train, epochs=200)\n",
    "network.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.80705159e-07]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the nodel\n",
    "variance = network.test(X_test, y_test) - network.test(X_train, y_train)\n",
    "print(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
